Rs40 = rexp(n, Lambda) # Generate the record set
hist(Rs40,
main="Histogram for randomly generated Exponential Distribution",
xlab = "Result"
) # generate the Histogram for the record set
TheoreticalMean = 1/Lambda
mean40 = mean(Rs40) # calculate Mean for this record set
abline(v = mean40, col = "blue", lwd = 2)
abline(v = TheoreticalMean, col = "red", lwd = 2)
n = 40 # Sample Size
Lambda = 0.2 # Lambda Value
Rs40 = rexp(n, Lambda) # Generate the record set
hist(Rs40,
main="Histogram for randomly generated Exponential Distribution",
xlab = "Result of `rexp(n, Lambda)`"
) # generate the Histogram for the record set
TheoreticalMean = 1/Lambda
mean40 = mean(Rs40) # calculate Mean for this record set
abline(v = mean40, col = "blue", lwd = 2)
abline(v = TheoreticalMean, col = "red", lwd = 2)
mns = NULL # create and reset a variable
for (i in 1 : 1000) mns = c(mns, mean(rexp(n, Lambda))) # run the simulation for 1000 times
hist(mns) # draw the histogram
mean1k = mean(mns) # calculate the mean for the result
abline(v = mean1k, col = "blue", lwd = 2)
abline(v = TheoreticalMean, col = "red", lwd = 2)
varTheory = (1/Lambda) / sqrt(n) # Calculate the Theoratical Variance
x_bar = 1100
s_std_dev = 30
n = 9
qt_val = 0.975
CI = x_bar + (c(-1,1) * qt(qt_val,n-1)) * (s_std_dev/sqrt(n))
n = 9
x_bar =  -2
qt_val = 0.975
s_std_dev = 2.6
CI = x_bar + (c(-1,1) * qt(qt_val,n-1)) * (s_std_dev/sqrt(n))
s_p = 0
n_a = 9
n_b = 9
s_a = 1.5
s_b = 1.8
qt_val = 0.95
x_bar_a = -3
x_bar_b = 1
sp2 = (((n_a - 1)*((s_a)^2)) + ((n_b - 1) * ((s_b)^2)))/(n_a + n_b - 2)
s_p = sqrt(sp2)
CI = 0
CI = (x_bar_a - x_bar_b) + c(-1,1) * qt(qt_val,(n_a + n_b - 2)) * s_p * sqrt((1/n_a)+(1/n_b))
s_p = 0
n_a = 9
n_b = 9
s_a = 1.5
s_b = 1.8
qt_val = 0.95
x_bar_a = -3
x_bar_b = 1
sp2 = (((n_a - 1)*((s_a)^2)) + ((n_b - 1) * ((s_b)^2)))/(n_a + n_b - 2)
s_p = sqrt(sp2)
CI = 0
CI = (x_bar_a - x_bar_b) + c(-1,1) * qt(qt_val,(n_a + n_b - 2)) * s_p * sqrt((1/n_a)+(1/n_b))
s_p = 0
n_a = 10
n_b = 10
s_a = 0.6
s_b = 0.68
qt_val = 0.975
x_bar_a = 3
x_bar_b = 5
sp2 = (((n_a - 1)*((s_a)^2)) + ((n_b - 1) * ((s_b)^2)))/(n_a + n_b - 2)
s_p = sqrt(sp2)
CI = 0
CI = (x_bar_a - x_bar_b) + c(-1,1) * qt(qt_val,(n_a + n_b - 2)) * s_p * sqrt((1/n_a)+(1/n_b))
bpBase = c(140,138,150,148,135)
bpW2 = c(132,135,151,146,130)
boxplot(bpBase,bpW2)
t.test(bpBase, bpW2, mu=0, alt = "two.side",paired = TRUE,conf = 0.95,var.equal = T)
mn <- 1,100
s <- 30
z <- qnorm(.05)
mu0 <- mn - z * s / sqrt(nrow(mtcars))
mn <- 1100
s <- 30
z <- qnorm(.05)
n = 9
mu0 <- mn - z * s / sqrt(n)
?t.test
t.test(num1, num2, mu=0, alt = "two.side",paired = TRUE,conf = 0.95,var.equal = T)
num1 = 1081
num2 = 1119
t.test(num1, num2, mu=0, alt = "two.side",paired = TRUE,conf = 0.95,var.equal = T)
num1 = 1081
num2 = 1119
t.test(num1, num2, mu=0, alt = "less",paired = TRUE,conf = 0.95,var.equal = T)
n <- 9
μ <- 1100
σ <- 30
quantile = 0.975 # is 95% with 2.5% on both sides of the range
confidenceInterval = μ + c(-1, 1) * qt(quantile, df=n-1) * σ / sqrt(n)
confidenceInterval
sampleN <- 9
Mu <- 1100
sd <- 30
quantile = 0.975 # is 95% with 2.5% on both sides of the range
confidenceInterval = Mu + c(-1, 1) * qt(quantile, df=sampleN-1) * sd / sqrt(sampleN)
confidenceInterval
rate <- 1/100
errors <- 10
days <- 1787
test <-  poisson.test(errors, T = days, r = rate, alt="less")
round(test$p.value,2)
n <- 4
x <- 3
test <- binom.test(x=x, n=n, alt="greater")
round(test$p.value,2)
library(swirl)
swirl()
fit <- lm(child ~ parent,galton)
fit$residuals
summary(fit)
mean(fit$residuals)
cov(fit$residuals,galton$parent)
ols.ic <- fit$coefficients
ols.ic <- fit$coef
ols.ic <- fit$coef
ols.ic <- fit$coef[ 1]
ols.slope < fit$coefficients[2]
ols.slope < fit$coef[2]
ols.slope <- fit$coef[2]
lhs - rhs
all.equal(lhs ~ rhs)
all.equal(lhs ~ rhs, galton)
all.equal(lhs,rhs)
varChild <- var(lhs,rhs)
varChild <- var(galton$child)
varRes <- fit$residuals
varRes <- var(fit$residuals)
ols.slope <- est(ols.slope,ols.ic)
varEst <- var(est(ols.shope,ols.ic))
varEst <- var(est(ols.slope,ols.ic))
all.equal(())
all.equal(
#Here are the vectors of variations or tweaks
)
all.equal()
all.equal(varChild,sum(varRes))
all.equal(varChild,varEst+varRes)
efit <- lm(accel ~ mag+dist, attenu)
mean(varRes)
mean(efit$residuals)
varRes <- fit$residuals
cov(efit$residuals,attenu$mag)
cov(efit$residuals,attenu$dist)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
fit <- lm(x~w)
str(fit)
fit$residuals^2
fit$fitted.values
fit <- lm(x~w,-1)
fit
fit <- lm(x~w)
fit
fit <- lm(x~w-1)
fit
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
fit <- lm(x~y-1)
fit
fit$coefficients
fit <- lm(y~x-1)
fit$coefficients
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
fit <- lm(w~x)
fit$fitted.values
fit <- lm(x~w)
fit$fitted.values
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
data("mtcars")
data("mtcars")
str(mtcars)
fit <- lm(mpt~wt,mtcars)
fit <- lm(mpg~wt,mtcars)
fit
fit <- lm(wt~mpg,mtcars)
fit
fit <- lm(mpg~wt,mtcars)
fit$coefficients
plot(wt,mpg)
plot(mtcars$wt,mtcars$mpg)
fit$coefficients
x <- c(0.18, -1.54, 0.42, 0.95)
mean(x)
fit
lm((mpg-mean(mpg))~(wt-mean(wt))-1,mtcars)
lm(I(mpg-mean(mpg))~I(wt-mean(wt))-1,mtcars)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
fit <- lm(x~y)
fit
fit <- lm(y~x)
fit
fit$coefficients
str(fit)
mean(x)
x2 <- c(8.58, 10.46, 9.01, 9.64, 8.86)
mean(x2)
(1/1.5)*.5
5/4
.5/1/25
.5/1.25
(1/.5)*.5
x <- c(0.18, -1.54, 0.42, 0.95)
plot(x~w)
plot(w~x)
mean(x2)
fit <- lm(w~x)
summary(fit)
fit <- lm(x~w)
summary(fit)
mean(x)
mean(w)
mean(x)*mean(w)
mean(x*w)
swirl()
cor(gch_nor,gpa_nor)
l_nor <- lm(galton$parent ~ galton$child)
l_nor <- lm(gch_nor ~ ggpa_nor)
l_nor <- lm(gch_nor ~ gpa_nor)
4
swirl()
ones <- rep(1,nrow(galton))
lm(child ~ ones + parent -1,galton)
lm(child ~ parent, galton)
lm(child~1,galton)
view(trees)
View(trees)
fit <- lm(Volume ~ Girth + Height + Constant -1,trees)
trees2 <- eliminate("Girth",trees)
head(trees2)
fit2 <- lm(Volume ~ Height + Constant -1, trees2)
lapply(list(fit,fit2),coef)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lim(y~x)
fit <- lm(y~x)
summary(fit)
fit$coefficients
summary(fit)$sigma
data("mtcars")
fit <- lm(mpg~wt,mtcars)
newdata <- data.frame(wt=mean(mtcars$wt))
predict(fit,newdata,interval = "confidence")
?mtcars
newdata <- data.frame(wt = 3000/1000)
predict(fit,newdata,interval = "confidence")
predict(fit,newdata,interval = "prediction")
newdata <- data.frame(wt = 2000/1000)
predict(fit,newdata,interval = "prediction")
rm(list = ls())
library(dplyr)
library(data.table)
library(graphics)
library(ggplot2)
data(mtcars)
mtcars$cyl <- as.factor(mtcars$cyl)
mtcars$vs <- as.factor(mtcars$vs)
mtcars$am <- factor(mtcars$am)
mtcars$gear <- factor(mtcars$gear)
mtcars$carb <- factor(mtcars$carb)
attach(mtcars)
pair(mtcars)
pairs(mtcars)
pairs(mtcars)
coplot(mpg ~ disp | as.factor(cyl), data = mtcars,
panel = panel.smooth, rows = 1)
coplot(mpg ~ disp | as.factor(am), data = mtcars,
panel = panel.smooth, rows = 1)
data(mtcars)
mtcars$cyl <- as.factor(mtcars$cyl)
mtcars$vs <- as.factor(mtcars$vs)
mtcars$am <- factor(mtcars$am)
mtcars$gear <- factor(mtcars$gear)
mtcars$carb <- factor(mtcars$carb)
attach(mtcars)
boxplot(mpg ~ am, xlab="Transmission (0 = Automatic, 1 = Manual)", ylab="MPG",
main="Boxplot for MPG vs Transmission")
main="Boxplot for MPG vs Transmission Type")
boxplot(mpg ~ am, xlab="Transmission (0 = Automatic, 1 = Manual)", ylab="MPG",
main="Boxplot for MPG vs Transmission Type")
main="Miles-Per-Galon vs Transmission Type")
boxplot(mpg ~ am, xlab="Transmission (0 = Automatic, 1 = Manual)", ylab="MPG",
main="Miles-Per-Galon vs Transmission Type")
pairs(mtcars)
fitAll <- lm(mpg ~ .,data = mtcars)
fitStep <- step(fitAll,direction = "both")
summary(fitAll)
summary(fitStep)
summary(fit1)
fit1 <- lm(mpg ~ am, data=mtcars)
fit2 <- lm(mpg ~ disp + hp + wt, data=mtcars)
summary(fit1)
summary(fit2)
fit2 <- lm(mpg ~ am + disp + hp + wt, data=mtcars)
summary(fit2)
anova(fit1,fit2,fitAll,fitStep)
fit1 <- lm(mpg ~ am, data=mtcars)
fit2 <- lm(mpg ~ disp + hp + wt)
fit3 <- lm(mpg ~ am + disp + hp + wt, data=mtcars)
summary(fit1)
summary(fit2)
summary(fit3)
anova(fit1,fit2,fit3)
anova(fit2,fit3)
anova(fit1,fit2,fit3,fitAll,fitStep)
par(mfrow=c(2,2))
plot(fit2)
plot(fit3)
plot(fitAll)
anova(fit1,fit2,fit3,fitAll,fitStep)
summary(fit3)
fitHpWt <- lm(mpg ~ hp + wt, data = mtcars)
summary(fitHpWt)
fit1 <- lm(mpg ~ am, data=mtcars)
fit2 <- lm(mpg ~ hp + wt, data = mtcars)
# Models that include Predictors with obvious relationship as shown in the Pairs graph
fit3 <- lm(mpg ~ disp + hp + wt)
# Models that includes "AM" along with the previous Predictor
fit4 <- lm(mpg ~ am + disp + hp + wt, data=mtcars)
# Models that includes All characteristics as Predictor
fitAll <- lm(mpg ~ .,data = mtcars)
# Models using Step function to automatically select Predictor
fitStep <- step(fitAll,direction = "both")
anova(fit1,fit2,fit3,fit4,fitAll,fitStep)
par(mfrow=c(2,2))
plot(fit2)
plot(fit3)
plot(fit4)
anova(fit1,fit3,fit4,fitAll,fitStep)
fit2,
anova(fit1,fit2,fit3,fit4,fitAll,fitStep)
plot(fit2)
fitAll <- lm(mpg ~ .,data = mtcars)
# Models using Step function to automatically select Predictor
fitStep <- step(fitAll,direction = "both")
summary(fitStep)
plot(fitStep)
pairs(mtcars)
fit1 <- lm(mpg ~ am, data=mtcars)
#
fit2 <- lm(mpg ~ hp + wt, data = mtcars)
# Models that include Predictors with obvious relationship as shown in the Pairs graph
fit3 <- lm(mpg ~ disp + hp + wt)
# Models that includes "AM" along with the previous Predictor
fit4 <- lm(mpg ~ cyl + hp + wt + am, data=mtcars)
# Models that includes All characteristics as Predictor
fitAll <- lm(mpg ~ .,data = mtcars)
# Models using Step function to automatically select Predictor
fitStep <- step(fitAll,direction = "both")
# Summarize the Step function to
summary(fitStep)
# We use anova to compare nest
anova(fit1,fit2,fit3,fit4,fitAll,fitStep)
summary(fitStep)
summary(fit2)
summary(fit2)$coefficient
summary(fit2)
summary(fit2)$signif
summary(fit2)$coefficient
anova(fit1,fit2,fit3,fit4,fitAll,fitStep)
summary(fitStep)
summary(fit2)
par(mfrow=c(2,2))
plot(fit2, main = "Model Diagnostic Graph for MPG prediction")
summary(fit1)
fit2 <- lm(mpg ~ hp + wt, data = mtcars)
summary(fit2)
summary(fit4)
summary(fit3)
fit3 <- lm(mpg ~ am + hp + wt)
summary(fit3)
fit4 <- lm(mpg ~ cyl + hp + wt + am, data=mtcars)
summary(fit4)
plot(fit2)
hatvalues(fit2)
dfbetas(fit2)
fit1 <- lm(mpg ~ am, data=mtcars)
# Model that uses hp and wt as Predictors
fit2 <- lm(mpg ~ hp + wt, data = mtcars)
# Models that include Predictors with obvious relationship as shown in the Pairs graph
fit3 <- lm(mpg ~ disp + hp + wt)
# Models that includes "AM" along with the previous Predictor
fit4 <- lm(mpg ~ cyl + hp + wt + am, data=mtcars)
# Models that includes All characteristics as Predictor
fitAll <- lm(mpg ~ .,data = mtcars)
# Models using Step function to automatically select Predictor
fitStep <- step(fitAll,direction = "both")
plot(fit4)
plot(fit3)
fit <- lm(mpg ~ cyl + wt, data = mtcars)
fit
summary(fit)
install.packages(caret)
install.packages("caret")
dsTrain <- read.csv("pml-training.csv",na.string=c("NA", "#DIV/0!"))
dsTrain <- read.csv("\pml-training.csv",na.string=c("NA", "#DIV/0!"))
this.dir <- "C:/Users/User/Documents/R/PracticalMachineLearning.Assignment"
setwd(this.dir)
library(caret)
set.seed(3837)
```
# Import Data
```{r import data, echo=FALSE}
dsTrain <- read.csv("\pml-training.csv",na.string=c("NA", "#DIV/0!"))
dsTrain <- read.csv("pml-training.csv",na.string=c("NA", "#DIV/0!"))
View(dsTrain)
dsTrain[1:100,"classe"]
dsTrain[1100:1200,"classe"]
dsTrain[11000:11200,"classe"]
testonly <- dsTrain[11000:11200,"classe":]
testonly <- dsTrain[11000:11200,"classe"]
testonly <- dsTrain[11000:11200,1:10]
View(testonly)
testonly <- dsTrain[11000:11200,11:20]
dsTrainClean <-dsTrain[,-(1:7)]
dsTrainClean <-dsTrainClean[, apply(dsTrainClean, 2, function(x) !any(is.na(x)))]
View(dsTrainClean)
dsPartition <- createDataPartition(y=dsTrainClean$classe, p=0.70,list=F)
dsTrainCleanTrain <- dsTrainClean[dsPartition,]
dsTrainCleanTest <- dsTrainClean[-dsPartition,]
dsModelTree <- rpart(classe ~., data=dsTrainCleanTrain, method="class")
install.packages("rpart")
install.packages("randomForest")
library("rpart")
dsModelTree <- rpart(classe ~., data=dsTrainCleanTrain, method="class")
dsPredTree  <- predict(dsModelTree, dsTrainCleanTest, type="class")
confusionMatrix(dsPredTree,dsTrainCleanTest$classe)
confusionMatrix(dsPredTree,dsTrainCleanTest$classe)
install.packages("e1071")
confusionMatrix(dsPredTree,dsTrainCleanTest$classe)
dsModelRF <- randomForest(classe ~ ., data = dsTrainCleanTrain, ntree = 1024)
library("randomForest")
dsModelRF <- randomForest(classe ~ ., data = dsTrainCleanTrain, ntree = 1024)
dsPredRF <- predict(dsModelRF, dsTrainCleanTest, type="class")
confusionMatrix(dsPredRF,dsTrainCleanTest$classe)
install.packages("ipred")
dsModelBag <- bagging(classe ~., data=dsTrainCleanTrain, coob=TRUE)
dsPredBag <- predict(dsModelBag, dsTrainCleanTest)
library("ipred")
dsModelBag <- bagging(classe ~., data=dsTrainCleanTrain, coob=TRUE)
dsPredBag <- predict(dsModelBag, dsTrainCleanTest)
confusionMatrix(dsPredBag, dsTrainCleanTest$classe)
dsTestClean <- dsTest[,-(1:7)]
dsTest <- read.csv("pml-testing.csv",na.string=c("NA", "#DIV/0!"))
dsTestClean <- dsTest[,-(1:7)]
dsTestClean <- dsTestClean[,apply(dsTestClean, 2, function(x) !any(is.na(x)))]
dsResult <- predict(dsModelRF,dsTestClean)
dsResult
repeatCV3 = trainControl(method="repeatcv",number=10,repeat=3)
repeatCV3 = trainControl(method="repeatedcv",number=10,repeat=3)
repeatCV3 <- trainControl(method="repeatedcv",number=10,repeat=3)
repeatCV3 <- trainControl(method="repeatedcv",number=10,repeats=3)
dsModelTree <- train(classe~., data=training, method="tree",trControl=repeatCV3)
dsModelTree <- train(classe~., data=dsTrainCleanTrain, method="tree",trControl=repeatCV3)
dsModelTree <- train(classe~., data=dsTrainCleanTrain, method="rpart",trControl=repeatCV3)
#dsModelTree <- rpart(classe ~., data=dsTrainCleanTrain, method="class")
dsPredTree  <- predict(dsModelTree, dsTrainCleanTest, type="class")
repeatCV3 <- trainControl(method="repeatedcv",number=10,repeats=3,classProbs = TRUE)
dsModelTree <- train(classe~., data=dsTrainCleanTrain, method="rpart",trControl=repeatCV3)
dsPredTree  <- predict(dsModelTree, dsTrainCleanTest)
confusionMatrix(dsPredTree,dsTrainCleanTest$classe)
repeatCV3 <- trainControl(method="repeatedcv",classProbs = TRUE)
dsModelTree <- train(classe~., data=dsTrainCleanTrain, method="rpart",trControl=repeatCV3)
dsPredTree  <- predict(dsModelTree, dsTrainCleanTest)
confusionMatrix(dsPredTree,dsTrainCleanTest$classe)
repeatCV3 <- trainControl(method="cv",classProbs = TRUE)
dsModelTree <- train(classe~., data=dsTrainCleanTrain, method="rpart",trControl=repeatCV3)
dsPredTree  <- predict(dsModelTree, dsTrainCleanTest)
confusionMatrix(dsPredTree,dsTrainCleanTest$classe)
dsTrain <- read.csv("pml-training.csv")
View(dsTrain)
dsTrain <- read.csv("pml-training.csv",na.string=c("NA", "#DIV/0!"))
dim(dsTrain)
structure(dsTrain)
dim(dsTrain)
rm(list = ls())
this.dir <- "C:/Users/User/Documents/R/PracticalMachineLearning.Assignment"
setwd(this.dir)
library(caret)
set.seed(3837)
dsTrain <- read.csv("pml-training.csv",na.string=c("NA", "#DIV/0!"))
# Read the data into a Dataset and convert "NA" and "Div/0" as na
dsTest <- read.csv("pml-testing.csv",na.string=c("NA", "#DIV/0!"))
dim(dsTrain)
prop.table(table(is.na(dsTrain)))
dsTrainClean <- dsTrain[,-(1:7)]
dsTrainClean <- dsTrainClean[, apply(dsTrainClean, 2, function(x) !any(is.na(x)))]
dsTestClean <- dsTest[,-(1:7)]
dsTestClean <- dsTestClean[,apply(dsTestClean, 2, function(x) !any(is.na(x)))]
dsPartition <- createDataPartition(y=dsTrainClean$classe, p=0.70,list=F)
dsTrainCleanTrain <- dsTrainClean[dsPartition,]
dsTrainCleanValid <- dsTrainClean[-dsPartition,]
ModelTree <- rpart(classe ~., data=dsTrainCleanTrain, method="class")
PredTree  <- predict(ModelTree, dsTrainCleanValid)
confusionMatrix(PredTree,dsTrainCleanValid$classe)
ModelTree <- rpart(classe ~., data=dsTrainCleanTrain, method="class")
PredTree  <- predict(ModelTree, dsTrainCleanValid)
confusionMatrix(PredTree,dsTrainCleanValid$classe)
PredTree  <- predict(ModelTree, dsTrainCleanValid, type="class")
confusionMatrix(PredTree,dsTrainCleanValid$classe)
Random Tree and Random Forest Method is selected to perform Traning and Prediction. From the result, Random Forest gives the best result with 99.5% prediction accuracy with only _0.5% Out of Sampling error_ when _Cross Validated_ with the Validation Dataset. It was then used to predict the result on the Testing Dataset.
